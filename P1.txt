1.Parallelize - 
val rdd = sc.parallelize(Array(1,2,3,4,5,6,7,8,9,10)) 
rdd.foreach(f=>{ println(f)}) 

2.Read Text File - 
val rdd2 = spark.read.textFile("C:/MYfolder/sem/sem5/DE/Practicals/Files/a1.txt")
rdd2.foreach(f=>{ println(f)})
val rdd = spark.sparkContext.textFile("C:/MYfolder/sem/sem5/DE/Practicals/Files/*.txt")
rdd.foreach(f=>{ println(f)})
val rdd2 = spark.sparkContext.wholeTextFiles("C:/MYfolder/sem/sem5/DE/Practicals/Files/*")
rdd2.foreach(f=>{println(f._1+"=>"+f._2)})
val rdd3 = spark.sparkContext.textFile("C:/MYfolder/sem/sem5/DE/Practicals/Files/a1.txt,C:/MYfolder/sem/sem5/DE/Practicals/Files/a2.txt")
rdd3.foreach(f=>{println(f)})

3.Read CSV - 
val rdd5 = spark.sparkContext.textFile("C:/Users/admin/Desktop/files/*") 
val rdd6 = rdd5.map(f=>{f.split(",")}) 
rdd6.foreach(f => {println("Col1:"+f(0)+",Col2:"+f(1))}) 

4.Create RDD -
val rdd4 = spark.sparkContext.parallelize(Seq(("Java",20000),("Python",100)))
rdd4.foreach(println)
//Creating from another RDD
val rdd3 = rdd4.map(row=>{(row._1,row._2+100)})
rdd3.foreach(println)

5.Actions - 
val listRdd = spark.sparkContext.parallelize(List(1,2,3,4,5,3,2)) 
//collect() -Return the complete dataset as an Array
val data:Array[Int] = listRdd.collect() 
data.foreach(println) 
//count() – Return the count of elements in the dataset
println("Count : "+listRdd.count) 
//first() – Return the first element in the dataset
println("first : "+listRdd.first()) 

6.Pair functions - 
val rdd = spark.sparkContext.parallelize(List("Germany India USA","USA India Russia","India Brazil Canada China")) 
val wordsRdd = rdd.flatMap(_.split(" ")) 
val pairRDD = wordsRdd.map(f=>(f,1)) 
pairRDD.foreach(println) 
//distinct() – Returns distinct keys
pairRDD.distinct().foreach(println) 

println("Sort by Key ==>") 
val sortRDD = pairRDD.sortByKey() 
sortRDD.foreach(println) 

7. Repartition and Coalesce - 
val rdd = spark.sparkContext.parallelize(Range(0,20)) 
println("From local[5]"+rdd.partitions.size) 
val rdd1 = spark.sparkContext.parallelize(Range(0,20), 6) 
println("parallelize : "+rdd1.partitions.size) 

8. Shuffle Partitions - 
val sc = spark.sparkContext 
val rdd:RDD[String] = sc.textFile("src/main/resources/test.txt") 
println("RDD Parition Count :"+rdd.getNumPartitions)
val rdd2 = rdd.flatMap(f=>f.split(" ")).map(m=>(m,1))

9. Broadcast Variables - 
val broadcastVar = sc.broadcast(Array(0, 1, 2, 3)) 
broadcastVar.value 

10. Accumulator Variables - 
val accum = sc.longAccumulator("SumAccumulator")
sc.parallelize(Array(1, 2, 3)).foreach(x => accum.add(x))
accum.value 


11. Convert RDD to DataFrame - 
import spark.implicits._
val data = Seq(("Java", "20000"), ("Python", "100000"), ("Scala", "3000")) 
val rdd = spark.sparkContext.parallelize(data) 
val dfFromRDD1 = rdd.toDF()
dfFromRDD1.printSchema() 
